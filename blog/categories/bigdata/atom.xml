<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: bigdata | Chouqin's Blog]]></title>
  <link href="http://chouqin.github.io/blog/categories/bigdata/atom.xml" rel="self"/>
  <link href="http://chouqin.github.io/"/>
  <updated>2013-10-21T21:38:47+08:00</updated>
  <id>http://chouqin.github.io/</id>
  <author>
    <name><![CDATA[chouqin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[MapReduce学习心得]]></title>
    <link href="http://chouqin.github.io/blog/2013/10/16/mapreduce/"/>
    <updated>2013-10-16T00:00:00+08:00</updated>
    <id>http://chouqin.github.io/blog/2013/10/16/mapreduce</id>
    <content type="html"><![CDATA[<h2 id="section">简介</h2>

<p>对于计算机系的同学来说，MapReduce这个词应该并不陌生，
现在是所谓的“大数据时代”，“大数据”这个词被炒得非常热。
何为“大数据”？随着互联网的发展，现在的数据越来越多，
给原先的技术带来了两方面的挑战，一是<strong>存储</strong>，
如何存储这些PB级别的数据，
二是<strong>分析</strong>， 如何对这么大的数据进行分析，
从中提取出有用的信息。</p>

<p>MapReduce就是一个对大数据进行分析的框架。
使用MapReduce，用户只需要定义自己的<code>map</code>函数和<code>reduce</code>函数，
然后MapReduce就能把这些函数分配到不同的机器上去并行的执行，
MapReduce帮你解决好调度，容错，节点交互的问题。
这样，一个没有分布式系统编程经验的人也可以利用MapReduce把自己的程序放到几千台机器上去执行。</p>

<p><code>map</code>和<code>reduce</code>都是来自于函数式编程的概念，<code>map</code>函数接受一条条的纪录作为输入，
然后输出一个个<code>&lt;key, value&gt;</code>对，<code>reduce</code>函数接受<code>&lt;key, values&gt;</code>对，
（其中<code>values</code>是每一个<code>key</code>对应的所有的<code>value</code>)，通过对这些<code>values</code>进行一个“总结”，
得到一个<code>&lt;key, reduce_value&gt;</code>。</p>

<p>比如拿经典的<strong>WordCount</strong>例子来说，对于文本中的每一个单词<code>word</code>，
<code>map</code>都会生成一个<code>&lt;word, 1&gt;</code>对（注意如果一个文本中一个单词出现多次就生成多个这样的对）,
<code>reduce</code>函数就会收到<code>&lt;word, 1,...&gt;</code>这样的输入，它的工作就是把所有的<code>1</code>都加起来，
生成一个<code>&lt;word, sum&gt;</code>。</p>

<p>MapReduce函数基于键值对进行处理，看起来很简单，
那很多的分析任务不仅仅只是简单的Wordcount而已，能够使用这两个函数来实现吗？
幸运的是，很多的大规模数据分析任务都能通过MapReduce来表达，
这也是为什么MapReduce能够作为一个框架被提出的原因，
在最后一个部分中会给出一些更加复杂的使用MapReduce的例子。</p>

<h2 id="section-1">架构</h2>

<p>在大概了解了MapReduce之后，我们来看一下它到底是怎么实现的。
我们首先看一下一个MapReduce任务执行完需要经过那些流程，
然后再看一下在实现MapReduce的时候需要考虑的几个因素。</p>

<h3 id="section-2">基本流程</h3>

<p><img src="/images/map_reduce_execution.png" alt="" /></p>

<p>我们首先假定把一个任务分成M个<code>map</code>Task和R个<code>reduce</code>task，
如上图,
整个任务的执行过程如下：</p>

<ol>
  <li>
    <p>首先根据<code>map</code>的数量把原来的数据分成M个splits，每一个split对应一个<code>map</code>task。</p>
  </li>
  <li>
    <p>在集群的节点上启动<code>master</code>，M个<code>map</code>task和N个<code>reduce</code>task, 
<code>master</code>把split分配给相应的<code>map</code>task。需要注意的是，
一个集群的节点（又称作一个worker）上可能有多个<code>map</code>task,
也有可能<code>map</code>task和<code>reduce</code>task在同一个worker。</p>
  </li>
  <li>
    <p>每一个<code>map</code>task读取自己的split，根据用户定义的<code>map</code>函数生成<code>&lt;key value&gt;</code>对，
把结果保存在<strong>本地文件</strong>中，
根据<code>key</code>的不一样，结果被写入到R个不同的文件。
这些文件的位置会告知给<code>master</code>，然后<code>master</code>再告知给相应的<code>reduce</code>task。</p>
  </li>
  <li>
    <p>当一个<code>reduce</code>task被告知这些文件的位置时，它通过远程调用读取这些文件的内容，
当和这个<code>reduce</code>task相关的所有文件都被读到之后，它把这些内容按照<code>key</code>进行一个排序，
然后就能保证同一个<code>key</code>的所有<code>values</code>同时被传给<code>reduce</code>。</p>
  </li>
  <li>
    <p><code>reduce</code>task使用用户定义的<code>reduce</code>函数处理上述排序好的数据，
将最终的结果保存到一个<strong>Global File System</strong>（比如GFS），
这是为了保证数据的可靠性。</p>
  </li>
</ol>

<h3 id="section-3">文件的保存</h3>

<p>在上述的过程中，我们看到<code>map</code>task的结果被保存在本地，
而把<code>reduce</code>task的结果保存在具有可靠性保证的文件系统上。
这是因为<code>map</code>task产生的是中间结果，当这些结果被<code>reduce</code>之后，
就可以被扔掉，不需要备份，这样可以节约磁盘空间。
而<code>reduce</code>task产生的是最终结果，需要一定的可靠性保证。</p>

<h3 id="split">split的粒度</h3>

<p>在对一个任务进行划分时，需要考虑split的粒度：</p>

<ul>
  <li>
    <p>如果split太小，M就会很大，
<code>master</code>需要纪录的数据就会很多，
就会消耗很多<code>master</code>的内存。</p>
  </li>
  <li>
    <p>如果split太大，一方面调度不具有灵活性，
因为调度是以split为单位的，一个较大的task无法被分割放到其他空闲的worker上去执行。
另一方面无法利用<code>locality</code>进行调度，
因为<code>map</code>task的输入文件一般保存在分布式文件系统上，
<code>master</code>在调度时尽量把一个split分配到较近的节点上去执行，
如果split太大超过了一个文件block的大小，
这样可能两个block在不同的节点上，甚至跨了不同的机架，
这样无法利用<code>locality</code>了。</p>
  </li>
</ul>

<p>所以，在实际应用中，split的大小为一个block。</p>

<h3 id="section-4">容错</h3>

<p>由于MapReduce被分布到上千台机器上去执行，
错误是不可避免的。
MapReduc需要在节点发生故障时进行处理。</p>

<p>当一个节点发生故障，
在这个节点上的所有的<code>map</code>task都需要重新执行，
因为<code>map</code>task的结果是保存在节点本地的，
节点发生故障之后，这些结果就不可用了。
而成功执行的<code>reduce</code>task就不需要重新执行了，
因为它的结果是保存在分布式文件系统上，
可靠性是可以保证的。</p>

<h2 id="section-5">常见应用</h2>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h3 id="section-6">矩阵向量乘法</h3>

<p>假设有一个<script type="math/tex">m \times n</script>的矩阵M，
它和一个n维列向量v的乘积是一个m维的列向量x，有</p>

<script type="math/tex; mode=display">
x_i = \sum_{j=1}^{n} m_{ij}v_j
</script>

<p>可以根据j，把M按列分成k块，把v也对应分成k块，
每个M块和对应的v块被分给一个<code>map</code>task，
<code>map</code>task生成的结果为<script type="math/tex">(i, m_{ij}v_j)</script>对，
<code>reduce</code>task把每一个i对应的所有<script type="math/tex">m_{ij}v_j</script>加起来。</p>

<h3 id="section-7">关系代数运算</h3>

<p>关系数据库表的Join，Selection，Projection, Union, Intersection, 
Difference，Group And Aggregation等操作都可以使用MapReduce来实现。</p>

<p>值得一提的是，对于Join运算，比如链接关系R(a, b)和关系S(b, c)，
在生成以b为键的键值对时，需要指定来自于哪一个关系，
比如关系R生成的键值对的形式为<code>&lt;b, (R, a)&gt;</code>,
这样<code>reduce</code>时就可以根据这个信息进行组合。</p>

<h3 id="section-8">矩阵乘法</h3>

<p>假设有一个<script type="math/tex">m \times n</script>的矩阵M，
和一个<script type="math/tex">n \times p</script>的矩阵N，
它们的乘积是一个<script type="math/tex">m \times p</script>的矩阵P,
有:</p>

<script type="math/tex; mode=display">
p_{ik} = \sum_{j=1}^{n} m_{ij}n_{jk}
</script>

<p>矩阵M和矩阵N的乘法可以看成是关系M(I, J, V)和关系N(J, K, W)先进行一次Join，
再进行一次Group And Aggregation之后的结果，
因此可以直接通过两次MapReduce进行矩阵的乘法运算。</p>

<p>如果想要一次MapReduce就得到结果，可以在<code>map</code>时以(i, k)为键生成键值对，
同样的，需要指明来自于矩阵M还是矩阵N，因此，相应的键值对的格式分别为
<script type="math/tex">((i, k), (M, j, m_{ij}))</script>(对于矩阵M)，<script type="math/tex">((i, k), (N, j, n_{jk}))</script>(对于矩阵N)。
<code>reduce</code>时进行相应的组合。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GFS学习心得]]></title>
    <link href="http://chouqin.github.io/blog/2013/10/10/gfs/"/>
    <updated>2013-10-10T00:00:00+08:00</updated>
    <id>http://chouqin.github.io/blog/2013/10/10/gfs</id>
    <content type="html"><![CDATA[<p>又好久没写博客了，这几个月来零零散散做了一些事情，
学到的东西很杂，一直都没有形成系统，也没有在某些方面有很深的体会。
有些东西刚刚深入进去看了一点（比如<a href="redis.io">redis</a>，看过一些代码，下次一定要写点心得出来），
还没来得及总结，就被一些其他的事情把时间挤走了。而现在，
时间相对来说比较空闲，可以认真研究一些技术，学得一些东西了。</p>

<p>这几天一直在看<a href="http://research.google.com/archive/gfs.html">The Google File System</a>这篇论文，
看得很慢，一天才能看几页，有些地方还要反反复复看几遍才能“理解”，
但也确实学得了不少东西，包括一些分布式系统设计的基本思想，
以及如何根据应用的具体场景来做设计决策。
诚然，对于一个这么大的系统，要想弄明白它的全部细节是比较困难的，
能够把它的整个过程捋顺就可以了。本文中，
我把我对这篇论文印象比较深的内容用我自己的理解讲出来，
希望能够给对GFS感兴趣的同学一点帮助。</p>

<h2 id="section">特殊的应用场景</h2>

<p>GFS作为一个分布式的文件系统，
除了要满足一般的文件系统的需求之外，
还根据一些特殊的应用场景（原文反复提到的<code>application workloads and technological environment</code>），
来完成整个系统的设计。</p>

<h3 id="section-1">分布式文件系统的要求</h3>

<p>一般的分布式文件系统需要满足以下四个要求：</p>

<ul>
  <li>Performance：高性能，较低的响应时间，较高的吞吐量</li>
  <li>Scalability: 易于扩展，可以简单地通过增加机器来增大容量</li>
  <li>Reliability: 可靠性，系统尽量不出错误</li>
  <li>Availability: 可用性，系统尽量保持可用</li>
</ul>

<p>（注：关于reliability和availability的区别，
请参考<a href="http://unfolding-mirror.blogspot.com/2009/06/reliability-vs-availability.html">这篇</a>）</p>

<h3 id="gfs">GFS基于的假设</h3>

<p>基于对实际应用场景的研究，GFS对它的使用场景做出了如下假设：</p>

<ol>
  <li>
    <p>GFS运行在成千上万台便宜的机器上，这意味着节点的故障会经常发生。
必须有一定的容错的机制来应对这些故障。</p>
  </li>
  <li>
    <p>系统要存储的文件通常都比较大，每个文件大约100MB或者更大，
GB级别的文件也很常见。必须能够有效地处理这样的大文件，
基于这样的大文件进行系统优化。</p>
  </li>
  <li>
    <p>workloads的读操作主要有两种：</p>

    <ul>
      <li>
        <p>大规模的流式读取，通常一次读取数百KB的数据,
 更常见的是一次读取1MB甚至更多的数据。
 来自同一个client的连续操作通常是读取同一个文件中连续的一个区域。</p>
      </li>
      <li>
        <p>小规模的随机读取，通常是在文件某个随机的位置读取
 几个KB数据。
 对于性能敏感的应用通常把一批随机读任务进行排序然后按照顺序批量读取，
 这样能够避免在通过一个文件来回移动位置。（后面我们将看到，
 这样能够减少获取metadata的次数，也就减少了和master的交互）</p>
      </li>
    </ul>
  </li>
  <li>
    <p>workloads的写操作主要由大规模的，顺序的append操作构成。
一个文件一旦写好之后，就很少进行改动。因此随机的写操作是很少的，
所以GFS主要针对于append进行优化。</p>
  </li>
  <li>
    <p>系统必须有合理的机制来处理多个client并发写同一个文件的情况。
文件经常被用于生产者-消费者队列，需要高效地处理多个client的竞争。
正是基于这种特殊的应用场景，GFS实现了一个无锁并发append。</p>
  </li>
  <li>
    <p>利用高带宽比低延迟更加重要。基于这个假设，
可以把读写的任务分布到各个节点，
尽量保证每个节点的负载均衡，
尽管这样会造成一些请求的延迟。</p>
  </li>
</ol>

<!--more-->

<h2 id="section-2">架构</h2>

<p>下面我们来具体看一下GFS的整个架构。</p>

<p><img src="/images/gfs-arch.png" alt="" /></p>

<p>可以看到GFS由三个不同的部分组成，分别是<code>master</code>，<code>client</code>, <code>chunkserver</code>。
<code>master</code>负责管理整个系统（包括管理metadata，垃圾回收等），一个系统只有一个<code>master</code>。
<code>chunkserver</code>负责保存数据，一个系统有多个<code>chunkserver</code>。
<code>client</code>负责接受应用程序的请求，通过请求<code>master</code>和<code>chunkserver</code>来完成读写等操作。
由于系统只有一个<code>master</code>，<code>client</code>对<code>master</code>请求只涉及metadata，
数据的交互直接与<code>chunkserver</code>进行，这样减小了<code>master</code>的压力。</p>

<p>一个文件由多个chunk组成，一个chunk会在多个<code>chunkserver</code>上存在多个replica。
对于新建文件，目录等操作，只是更改了metadata，
只需要和<code>master</code>交互就可以了。注意，与linux的文件系统不同，
目录不再以一个inode的形式保存，也就是它不会作为data被保存在<code>chunkserver</code>。
如果要读写文件的文件的内容，就需要<code>chunkserver</code>的参与，
<code>client</code>根据需要操作文件的偏移量转化为相应的<code>chunk index</code>，
向<code>master</code>发出请求，<code>master</code>根据文件名和<code>chunk index</code>，得到一个全局的<code>chunk handle</code>，
一个chunk由唯一的一个<code>chunk handle</code>所标识，
<code>master</code>返回这个<code>chunk handle</code>以及拥有这个chunk的<code>chunkserver</code>的位置。
（不止一个，一个chunk有多个replica，分布在不同的<code>chunkserver</code>。
必要的时候，<code>master</code>可能会新建chunk，
并在<code>chunkserver</code>准备好了这个chunk的replica之后，才返回）
<code>client</code>拿到<code>chunk handle</code>和<code>chunkserver</code>列表之后，
先把这个信息用文件名和<code>chunk index</code>作为key缓存起来，
然后对相应的<code>chunkserver</code>发出数据的读写请求。
这只是一个大概的流程，对于具体的操作过程，下面会做分析。</p>

<h3 id="chunk">Chunk大小</h3>

<p>Chunk的大小是一个值得考虑的问题。在GFS中，chunk的大小是64MB。
这比普通文件系统的block大小要大很多。
在<code>chunkserver</code>上，一个chunk的replica保存成一个文件，
这样，它只占用它所需要的空间，防止空间的浪费。</p>

<p>Chunk拥有较大的大小由如下几个好处:</p>

<ul>
  <li>它减少了<code>client</code>和<code>master</code>交互的次数。</li>
  <li>减少了网络的开销，由于一个客户端可能对同一个chunk进行操作，
这样可以与<code>chunkserver</code>维护一个长TCP连接。</li>
  <li>chunk数目少了，metadata的大小也就小了，这样节省了<code>master</code>的内存。</li>
</ul>

<p>大的chunk size也会带来一个问题，一个小文件可能就只占用一个chunk，
那么如果多个<code>client</code>同时操作这个文件的话，就会变成操作同一个chunk，
保存这个chunk的<code>chunkserver</code>就会称为一个hotspot。
这样的问题对于小的chunk并不存在，因为如果是小的chunk的话，
一个文件拥有多个chunk，操作同一个文件被分布到多个<code>chunkserver</code>.
虽然在实践中，可以通过错开应用的启动的时间来减小同时操作一个文件的可能性。</p>

<h3 id="metadata">Metadata</h3>

<p>GFS的<code>master</code>保存三种metadata：</p>

<ol>
  <li>文件和chunk的namespace</li>
  <li>文件到chunk的映射</li>
  <li>每一个chunk的具体位置</li>
</ol>

<p>metadata保存在内存中，可以很快地获取。
前面两种metadata会通过operation log来持久化。
第3种信息不用持久化，因为在<code>master</code>启动时，
它会问<code>chunkserver</code>要chunk的位置信息。
而且chunk的位置也会不断的变化，比如新的<code>chunkserver</code>加入。
这些新的位置信息会通过日常的<code>HeartBeat</code>消息由<code>chunkserver</code>传给<code>master</code>。</p>

<p>将metadata保存在内存中能够保证在<code>master</code>的日常处理中很快的获取metadata，
为了保证系统的正常运行，<code>master</code>必须定时地做一些维护工作，比如清除被删除的chunk，
转移或备份chunk等，这些操作都需要获取metadata。
metadata保存在内存中有一个不好的地方就是能保存的metadata受限于<code>master</code>的内存，
不过足够大的chunk size和使用前缀压缩，能够保证metadata占用很少的空间。</p>

<p>对metadata进行修改时，使用锁来控制并发。需要注意的是，对于目录，
获取锁的方式和linux的文件系统有点不太一样。在目录下新建文件，
只获取对这个目录的读锁，而对目录进行snapshot，却对这个目录获取一个写锁。
同时，如果涉及到某个文件，那么要获取所有它的所有上层目录的读锁。
这样的锁有一个好的地方是可以在通过一个目录下同时新建两个文件而不会冲突，
因为它们都是获得对这个目录的读锁。</p>

<h3 id="operation-log">Operation Log</h3>

<p>Operation log用于持久化存储前两种metadata，这样<code>master</code>启动时，
能够根据operation log恢复metadata。同时，可以通过operation log知道metadata修改的顺序，
对于重现并发操作非常有帮助。因此，必须可靠地存储operation log，
只有当operation log已经存储好之后才向<code>client</code>返回。
而且，operation log不仅仅只保存在<code>master</code>的本地，而且在远程的机器上有备份，
这样，即使<code>master</code>出现故障，也可以使用其他的机器做为<code>master</code>。</p>

<p>从operation log恢复状态是一个比较耗时的过程，因此，使用checkpoint来减小operation log的大小。
每次恢复时，从checkpoint开始恢复，只处理checkpoint只有的operation log。
在做checkpoint时，新开一个线程进行checkpoint，原来的线程继续处理metadata的修改请求，
此时把operation log保存在另外一个文件里。</p>

<h3 id="section-3">一致性模型</h3>

<p>关于一致性，先看几个定义，对于一个file region，存在以下几个状态：</p>

<ul>
  <li>consistent。如果任何replica, 包含的都是同样的data。</li>
  <li>defined。defined一定是consistent，而且能够看到一次修改造成的结果。</li>
  <li>undefined。undefined一定是consistent，是多个修改混合在一块。举个例子，
修改a想给文件添加A1,A2，修改b想给文件添加B1,B2，如果最后的结果是A1,A2,B1,B2，
那么就是defined，如果是A1,B1,A2,B2，就是undefined。</li>
  <li>inconsitent。对于不同的replica，包含的是不同的data。</li>
</ul>

<p>在GFS中，不同的修改可能会出现不同的状态。对于文件的append操作（是GFS中的主要写操作），
通过放松一定的一致性，更好地支持并发，在下面的具体操作时再讲述具体的过程。</p>

<h3 id="lease">Lease机制</h3>

<p><code>master</code>通过lease机制把控制权交给<code>chunkserver</code>，当写一个chunk时，
<code>master</code>指定一个包含这个chunk的replica的<code>chunkserver</code>作为<code>primary replica</code>，
由它来控制对这个chunk的写操作。一个lease的过期时间是60秒，如果写操作没有完成，
<code>primary replica</code>可以延长这个lease。<code>primary replica</code>通过一个序列号控制对这个chunk的写的顺序，
这样能够保证所有的replica都是按同样的顺序执行同样的操作，也就保证了一致性。</p>

<h3 id="section-4">版本号</h3>

<p>对于每一个chunk的修改，chunk都会赋予一个新的版本号。
这样，如果有的replica没有被正常的修改（比如修改的时候当前的<code>chunkserver</code>挂了）,
那么这个replica就被<code>stale replica</code>，当<code>client</code>请求一个chuck时，<code>stale replica</code>会被<code>master</code>忽略，
在<code>master</code>的定时管理过程中，会把<code>stale replica</code>删除。</p>

<h3 id="section-5">负载均衡</h3>

<p>为了尽量保证所有<code>chunkserver</code>都承受差不多的负载，
<code>master</code>通过以下机制来完成：</p>

<ul>
  <li>首先，在新建一个chunk或者是复制一个chunk的replica时，
尽量保证负载均衡。</li>
  <li>当一个chunk的replica数量低于某个值时，尝试给这个chuck复制replica</li>
  <li>扫描整个系统的分布情况，如果不够平衡，则通过移动一些replica来达到负责均衡的目的。</li>
</ul>

<p>注意，<code>master</code>不仅考虑了<code>chunkserver</code>的负载均衡，也考虑了机架的负载均衡。</p>

<h2 id="section-6">基本操作</h2>

<h3 id="read">Read</h3>

<p>Read操作其实已经在上面的Figure 1中描述得很明白了，有如下几个过程：</p>

<ol>
  <li>
    <p><code>client</code>根据chunk size的大小，把<code>(filename,byte offset)</code>转化为<code>(filename,chunk index)</code>,
发送<code>(filename,chunk index)</code>给<code>master</code></p>
  </li>
  <li>
    <p><code>master</code> 返回<code>(chunk handle,所有正常replica的位置)</code>, 
<code>client</code>以<code>(filename,chunk index)</code>作为key缓存这个信息</p>
  </li>
  <li>
    <p><code>client</code>发<code>(chunk handle,byte range)</code>给其中一个<code>chunkserver</code>，通常是最近的一个。</p>
  </li>
  <li>
    <p><code>chunkserver</code>返回chunk data</p>
  </li>
</ol>

<h3 id="overwrite">Overwrite</h3>

<p>直接假设<code>client</code>已经知道了要写的chunk，如Figure 2，具体过程如下:</p>

<p><img src="/images/gfs-write.png" alt="" /></p>

<ol>
  <li><code>client</code>向<code>master</code>询问拥有这个chunk的lease的<code>primary replica</code>，如果当前没有<code>primary replica</code>，
<code>master</code>把lease给其中的replica</li>
  <li><code>master</code>把<code>primary replica</code>的位置和其他的拥有这个chunk的replica的<code>chunkserver</code>（<code>secondary replica</code>）的位置返回，
<code>client</code>缓存这个信息。</li>
  <li><code>client</code>把数据以流水线的方式发送到所有的replica，流水线是一种最高效利用的带宽的方法，
每一个replica把数据用LRU buffer保存起来，并向<code>client</code>发送接受到的信息。</li>
  <li><code>client</code>向<code>primary replica</code>发送write请求，<code>primary replica</code>根据请求的顺序赋予一个序列号</li>
  <li><code>primary replica</code>根据序列号修改replica和请求其他的<code>secondary replica</code>修改replica，
这个统一的序列号保证了所有的replica都是按照统一的顺序来执行修改操作。</li>
  <li>当所有的<code>secondary replica</code>修改完成之后，返回修改完成的信号给<code>primary replica</code></li>
  <li><code>primary replica</code>向<code>client</code>返回修改完成的信号，如果有任何的<code>secondary replica</code>修改失败，
信息也会被发给<code>client</code>，<code>client</code>然后重新尝试修改，重新执行步骤3-7。</li>
</ol>

<p>如果一个修改很大或者到了chuck的边界，那么client会把它分成两个写操作，
这样就有可能发生在两个写操作之间有其他的写操作，所以这时会出现undefined的情况。</p>

<h3 id="record-append">Record Append</h3>

<p>Record Append的过程相对于Overwrite的不同在于它的错误处理不同，
当写操作没有成功时，<code>client</code>会尝试再次操作，由于它不知道offset，
所以只能再次append，这就会导致在一些replica有重复的记录，
而且不同的replica拥有不同的数据。</p>

<p>为了应对这种情况的发生，应用程序必须通过一定的校验手段来确保数据的正确性，
如果对于生产者-消费者队列，消费者可以通过唯一的id过滤掉重复的记录。</p>

<h3 id="snapshot">Snapshot</h3>

<p>Snapshot是对文件或者一个目录的“快照”操作，快速地复制一个文件或者目录。
GFS使用<em>Copy-on-Write</em>实现snapshot，首先<code>master</code>revoke所有相关chunk的lease，
这样所有的修改文件的操作都需要和<code>master</code>联系，
然后复制相关的metadata，复制的文件跟原来的文件指向同样的chunck，
但是chuck的reference count大于1。</p>

<p>当有<code>client</code>需要写某个相关的chunck C时，<code>master</code>会发现它的reference count大于1，
<code>master</code>推迟回复给<code>client</code>，先新建一个<code>chunk handle</code>C’，
然后让所有拥有C的replica的<code>chunkserver</code>在本地新建一个同样的C‘的replica，
然后赋予C’的一个replica一个lease，把C’返回给<code>client</code>用于修改。</p>

<h3 id="delete">Delete</h3>

<p>当<code>client</code>请求删除文件时，GFS并不立即回收这个文件的空间。
也就是说，文件相关的metadata还在，
文件相关的chunk也没有从<code>chunkserver</code>上删除。
GFS只是简单的把文件删除的operation log记下，
然后把文件重新命名为一个hidden name， 里面包含了它的删除时间。
在<code>master</code>的日常维护工作时，
它会把删除时间删除时间超过3天的文件从metadata中删除，
同时删除相应chunk的metadata，
这样这些chunk就变成了orphan chunk，
它们会在<code>chunkserver</code>和<code>master</code>进行<code>Heartbeat</code>交互时从<code>chunkserver</code>删除。</p>

<p>这样推迟删除（原文叫垃圾回收）的好处有：</p>

<ul>
  <li>对于分布式系统而言，要确保一个动作正确执行是很难的，
所以如果当场要删除一个chunk的所有replica需要复杂的验错，重试。
如果采用这种推迟删除的方法，只要metadata被正确的处理，最后的replica就一定会被删除，
非常简单</li>
  <li>把这些删除操作放在<code>master</code>的日常处理中，可以使用批处理这些操作，
平摊下来的开销就小了</li>
  <li>可以防止意外删除的可能，类似于回收站</li>
</ul>

<p>这样推迟删除的不好在于浪费空间，如果空间吃紧的话，<code>client</code>可以强制删除，
或者指定某些目录下面的文件直接删除。</p>

<h2 id="section-7">后记</h2>

<p>GFS，MapReduce，BigTable并称为Google的“三架马车”，
既然看了GFS，怎么能不看另外两篇？
欲知Mapreduce，BigTable到底是怎么一回事，
请静候我接下来的博文。</p>
]]></content>
  </entry>
  
</feed>
